{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "mount_file_id": "1ZRw0QkxMw9XzmRm9yDO8i_hrLsTq2LIW",
      "authorship_tag": "ABX9TyMUsnOyR9Vo9UDOvW5L6wxo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AzhakAnwar/CosmeticStoreProject/blob/main/ScrapingCosmeticStores.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import os, sys \n",
        "from IPython.core.interactiveshell import InteractiveShell\n",
        "import platform, logging \n",
        "\n",
        "# try:\n",
        "#     import scrapy\n",
        "# except:\n",
        "#     ! pip3 install scrapy # type: ignore\n",
        "#     import scrapy\n",
        "\n",
        "# from scrapy.crawler import CrawlerProcess, CrawlerRunner \n",
        "# from twisted.internet import reactor\n",
        "from multiprocessing import Process, Queue\n",
        "\n",
        "# import subprocess "
      ],
      "metadata": {
        "id": "ugVye6H7_ACL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "InteractiveShell.ast_node_interactivity = \"all\"\n",
        "platform.python_version()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "9_c2TDFoEkI7",
        "outputId": "82959cdb-876f-4509-dd1b-5f45e5ecc9c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'3.8.10'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "class JsonWriterPipeline(object):\n",
        "\n",
        "    def open_spider(self, spider):\n",
        "        self.file = open('cosmeticstores.json', 'w')\n",
        "\n",
        "    def close_spider(self, spider):\n",
        "        self.file.close()\n",
        "\n",
        "    def process_item(self, item, spider):\n",
        "        record = json.dumps(dict(item)) + \"\\n\"\n",
        "        self.file.write(record)\n",
        "        return item"
      ],
      "metadata": {
        "id": "2j2RbPjCEwaA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive/') "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7eUaQ8jFAe4R",
        "outputId": "8315ee1f-81fe-4a33-c31d-0a0cbd0191b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "scrapy_path = \"/content/drive/MyDrive/ScrapTest\"\n",
        "os.chdir(scrapy_path)\n"
      ],
      "metadata": {
        "id": "N2jE1oAu7zOr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lib_path = '/content/drive/MyDrive/Library'\n",
        "sys.path.insert(0,lib_path)\n",
        "\n",
        "# ! chmod 777 -R '/content/drive/MyDrive/Library/bin/scrapy'\n"
      ],
      "metadata": {
        "id": "Rh4tq58s9Pq0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import scrapy"
      ],
      "metadata": {
        "id": "3V8Z_FSqWoyz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    import scrapy\n",
        "except:\n",
        "    ! pip3 install scrapy # type: ignore\n",
        "    import scrapy\n",
        "\n",
        "from scrapy.crawler import CrawlerProcess, CrawlerRunner \n",
        "from twisted.internet import reactor"
      ],
      "metadata": {
        "id": "JZ3uHE_5WuTO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !scrapy startproject first_project $scrapy_path\n",
        "os.chdir('/content/drive/MyDrive/ScrapTest/first_project')\n",
        "\n",
        "# !scrapy genspider cosmetic goschonheit.ch"
      ],
      "metadata": {
        "id": "JN-6tkaQFNk-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "class CosmeticSpider(scrapy.Spider):\n",
        "    name = 'cosmetic'\n",
        "    allowed_domains = ['goschonheit.ch']\n",
        "    # start_urls = ['http://goschonheit.ch/']\n",
        "\n",
        "    custom_settings = {\n",
        "        'CONCURRENT_REQUESTS': 5,\n",
        "        'ROBOTSTXT_OBEY': False,\n",
        "        'CONCURRENT_REQUESTS': 5,\n",
        "        'DOWNLOAD_DELAY': 3,\n",
        "        'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36',\n",
        "        'LOG_LEVEL': logging.WARNING,\n",
        "        # 'ITEM_PIPELINES': {'__main__.JsonWriterPipeline': 1}, # Used for pipeline 1\n",
        "        'FEED_FORMAT':'json',\n",
        "        'FEED_URI': 'cosmetic_stores.json',\n",
        "        'LOG_FILE': 'test_project.log',\n",
        "        'AUTOTHROTTLE_ENABLED': True,\n",
        "        'AUTOTHROTTLE_START_DELAY': 5,\n",
        "        'AUTOTHROTTLE_MAX_DELAY': 60\n",
        "    }\n",
        "    def start_requests(self):\n",
        "        yield scrapy.Request('http://goschonheit.ch/', callback=self.parse_city)\n",
        "        \n",
        "    def parse_city(self, response):\n",
        "        cities = response.xpath(\"//span[@class='li_inner']/a\")\n",
        "        for city in cities:\n",
        "            yield response.follow(city.xpath('./@href').get()+'?distance=99999', callback=self.open_entity, meta={'city': city.xpath('./text()').get()})\n",
        "            \n",
        "    def open_entity(self, response):\n",
        "        for shop in response.xpath(\"//h3/a/@href\").getall():\n",
        "            # print('Shop: ', shop)\n",
        "            yield response.follow(shop, callback=self.parse_entity, meta={'city': response.meta.get('city')})\n",
        "            \n",
        "        nxt = response.xpath(\"//div[@class='pagination']/a[contains(., 'chste')]/@href\").get()\n",
        "        if nxt:\n",
        "            yield response.follow(nxt, callback=self.open_entity)\n",
        "            \n",
        "    def parse_entity(self, response):\n",
        "        out = {\n",
        "            'name': response.xpath(\"//h1/text()\").get(),\n",
        "            'rating': response.xpath(\"(//span[@class='average'])[1]/text()\").get(),\n",
        "            'address': response.xpath(\"//div[@class='adr']/text()\").getall(),\n",
        "            'phone': response.xpath(\"//div[@itemprop='telephone']/text()\").get(),\n",
        "            'website': response.xpath(\"//div[@class='ca_content_info'][1]/div[last()]/text()\").get(),\n",
        "            'city': response.meta.get('city'),\n",
        "        }\n",
        "        yield out\n"
      ],
      "metadata": {
        "id": "NRQZh5OnAASU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def exec_spider(spider):\n",
        "    def fork(q):\n",
        "        try:\n",
        "            runner = CrawlerRunner()\n",
        "            deferred = runner.crawl(spider)\n",
        "            deferred.addBoth(lambda _: reactor.stop())\n",
        "            reactor.run()\n",
        "            q.put(None)\n",
        "        except Exception as e:\n",
        "            q.put(e)\n",
        "\n",
        "    q = Queue()\n",
        "    p = Process(target=fork, args=(q,))\n",
        "    p.start()\n",
        "    result = q.get()\n",
        "    p.join()"
      ],
      "metadata": {
        "id": "NWHk6ShtT3cV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "exec_spider(CosmeticSpider)\n"
      ],
      "metadata": {
        "id": "6_ZLwDyhVlxN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ! scrapy crawl cosmetic -o out.csv\n",
        "\n",
        "process = CrawlerProcess()\n",
        "process.crawl(CosmeticSpider)\n",
        "\n",
        "if reactor.running:\n",
        "    reactor.stop()\n",
        "\n",
        "\n",
        "process.start()"
      ],
      "metadata": {
        "id": "A3T_0BYxGHUv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}