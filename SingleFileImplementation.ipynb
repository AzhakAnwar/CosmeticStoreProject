{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1ZRw0QkxMw9XzmRm9yDO8i_hrLsTq2LIW",
      "authorship_tag": "ABX9TyNgUoyK/uGE11usdL+LVrnV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AzhakAnwar/CosmeticStoreProject/blob/main/SingleFileImplementation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Deploying Scrapy On Google Colab\n",
        "\n",
        "---\n",
        "Contain the following major features:\n",
        "\n",
        "\n",
        "\n",
        "1.   Deployed onn Google Colab.\n",
        "2.   Implemented with both, command line and Single File.\n",
        "3. The data is stored in Google Drive and Read/Write is made via mount.\n",
        "4. The library is installed on Google Drive and is imported from there making it a completely flexible and portable project. \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "r5U2OQjYj5wh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Necessary Imports"
      ],
      "metadata": {
        "id": "_1CZohubka3H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import os, sys \n",
        "from IPython.core.interactiveshell import InteractiveShell\n",
        "import platform, logging \n",
        "\n",
        "# try:\n",
        "#     import scrapy\n",
        "# except:\n",
        "#     ! pip3 install scrapy # type: ignore\n",
        "#     import scrapy\n",
        "\n",
        "# from scrapy.crawler import CrawlerProcess, CrawlerRunner \n",
        "# from twisted.internet import reactor\n",
        "from multiprocessing import Process, Queue\n",
        "\n",
        "# import subprocess "
      ],
      "metadata": {
        "id": "ugVye6H7_ACL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "IPython shell to display all values of statements in the output, instead of only the last one by default."
      ],
      "metadata": {
        "id": "uK8bPto0kt0G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "InteractiveShell.ast_node_interactivity = \"all\""
      ],
      "metadata": {
        "id": "9_c2TDFoEkI7"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Mount the Google Drive (In case it's not mounted already)"
      ],
      "metadata": {
        "id": "NyVYNIUAlInD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive/') "
      ],
      "metadata": {
        "id": "7eUaQ8jFAe4R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scrapy_path = \"/content/drive/MyDrive/ScrapTest\"  ## Project Path\n",
        "os.chdir(scrapy_path)\n"
      ],
      "metadata": {
        "id": "N2jE1oAu7zOr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lib_path = '/content/drive/MyDrive/Library'       ## Path where Scrapy Library is installed \n",
        "sys.path.insert(0,lib_path)\n",
        "\n",
        "# ! chmod 777 -R '/content/drive/MyDrive/Library/bin/scrapy'\n"
      ],
      "metadata": {
        "id": "Rh4tq58s9Pq0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing Scrapy Now As It's Installed In Google Drive"
      ],
      "metadata": {
        "id": "Qb7xf6IZlgWR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    import scrapy\n",
        "except:\n",
        "    ! pip3 install scrapy # type: ignore\n",
        "    import scrapy\n",
        "\n",
        "from scrapy.crawler import CrawlerProcess, CrawlerRunner \n",
        "from twisted.internet import reactor"
      ],
      "metadata": {
        "id": "JZ3uHE_5WuTO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !scrapy startproject cosmetics_project $scrapy_path\n",
        "os.chdir('/content/drive/MyDrive/ScrapTest/cosmetics_project')\n",
        "\n",
        "# !scrapy genspider cosmetic goschonheit.ch"
      ],
      "metadata": {
        "id": "JN-6tkaQFNk-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Single File Implemenation "
      ],
      "metadata": {
        "id": "f5iJFkRil5hb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CosmeticSpider(scrapy.Spider):\n",
        "    name = 'cosmetic'\n",
        "    allowed_domains = ['goschonheit.ch']\n",
        "    # start_urls = ['http://goschonheit.ch/']\n",
        "\n",
        "    custom_settings = {     # Class attribute\n",
        "        'CONCURRENT_REQUESTS': 5,\n",
        "        'ROBOTSTXT_OBEY': False,\n",
        "        'CONCURRENT_REQUESTS': 5,\n",
        "        'DOWNLOAD_DELAY': 3,\n",
        "        'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36',\n",
        "        'LOG_LEVEL': logging.WARNING,\n",
        "        # 'ITEM_PIPELINES': {'__main__.JsonWriterPipeline': 1}, # Used for pipeline 1\n",
        "        'FEED_FORMAT':'json',\n",
        "        'FEED_URI': 'cosmetic_stores.json',\n",
        "        'LOG_FILE': 'test_project.log',\n",
        "        'AUTOTHROTTLE_ENABLED': True,\n",
        "        'AUTOTHROTTLE_START_DELAY': 5,\n",
        "        'AUTOTHROTTLE_MAX_DELAY': 60\n",
        "    }\n",
        "    def start_requests(self):\n",
        "        yield scrapy.Request('http://goschonheit.ch/', callback=self.parse_city) \n",
        "        \n",
        "    def parse_city(self, response):\n",
        "        cities = response.xpath(\"//span[@class='li_inner']/a\")\n",
        "        for city in cities:                                  # follow city URL\n",
        "            yield response.follow(city.xpath('./@href').get()+'?distance=99999', callback=self.open_entity, meta={'city': city.xpath('./text()').get()})\n",
        "            \n",
        "    def open_entity(self, response):\n",
        "        for shop in response.xpath(\"//h3/a/@href\").getall(): # follow shop \n",
        "            yield response.follow(shop, callback=self.parse_entity, meta={'city': response.meta.get('city')})\n",
        "            \n",
        "        nxt = response.xpath(\"//div[@class='pagination']/a[contains(., 'chste')]/@href\").get()\n",
        "        if nxt:                                              # pagination \n",
        "            yield response.follow(nxt, callback=self.open_entity)\n",
        "            \n",
        "    def parse_entity(self, response):                        # Final output\n",
        "        out = {\n",
        "            'name': response.xpath(\"//h1/text()\").get(),\n",
        "            'rating': response.xpath(\"(//span[@class='average'])[1]/text()\").get(),\n",
        "            'address': response.xpath(\"//div[@class='adr']/text()\").getall(),\n",
        "            'phone': response.xpath(\"//div[@itemprop='telephone']/text()\").get(),\n",
        "            'website': response.xpath(\"//div[@class='ca_content_info'][1]/div[last()]/text()\").get(),\n",
        "            'city': response.meta.get('city'),\n",
        "        }\n",
        "        yield out\n"
      ],
      "metadata": {
        "id": "NRQZh5OnAASU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Defining a Pipeline to Store Data"
      ],
      "metadata": {
        "id": "CIrkULsYk4yg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "class JsonWriterPipeline(object):\n",
        "\n",
        "    def open_spider(self, spider):\n",
        "        self.file = open('cosmeticstores.json', 'w')\n",
        "\n",
        "    def close_spider(self, spider):\n",
        "        self.file.close()\n",
        "\n",
        "    def process_item(self, item, spider):\n",
        "        record = json.dumps(dict(item)) + \"\\n\"\n",
        "        self.file.write(record)\n",
        "        return item"
      ],
      "metadata": {
        "id": "2j2RbPjCEwaA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Spider Execution"
      ],
      "metadata": {
        "id": "NwSROt53m-zI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ! scrapy crawl cosmetic -o out.csv\n",
        "\n",
        "process = CrawlerProcess()\n",
        "process.crawl(CosmeticSpider)\n",
        "\n",
        "if reactor.running:\n",
        "    reactor.stop()\n",
        "\n",
        "\n",
        "process.start()"
      ],
      "metadata": {
        "id": "A3T_0BYxGHUv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A New Way to Execute (In Testing Phase)"
      ],
      "metadata": {
        "id": "EYjjcc9UnLlI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def exec_spider(spider):\n",
        "    def fork(q):\n",
        "        try:\n",
        "            runner = CrawlerRunner()\n",
        "            deferred = runner.crawl(spider)\n",
        "            deferred.addBoth(lambda _: reactor.stop())\n",
        "            reactor.run()\n",
        "            q.put(None)\n",
        "        except Exception as e:\n",
        "            q.put(e)\n",
        "\n",
        "    q = Queue()\n",
        "    p = Process(target=fork, args=(q,))\n",
        "    p.start()\n",
        "    result = q.get()\n",
        "    p.join()"
      ],
      "metadata": {
        "id": "NWHk6ShtT3cV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "exec_spider(CosmeticSpider)\n"
      ],
      "metadata": {
        "id": "6_ZLwDyhVlxN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}